#!/usr/bin/env -S sd nix shell
#!nix-shell -i python3 -p python3PackagesBin.transformers python3PackagesBin.torchaudio python3PackagesBin.python-telegram-bot
#! vim: syntax=python

# Telegram bot to transcribe audio files to text. Based on: https://github.com/alefiury/SE-R_2022_Challenge_Wav2vec2

from pathlib import Path
from argparse import ArgumentParser
import os
import logging
import gc
import traceback

logging.basicConfig(level=logging.INFO)
logging.getLogger("httpx").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)


from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC
import torch
import torchaudio

import telegram
from telegram import Update
from telegram.ext import Application, filters, MessageHandler
from telegram.constants import ChatAction

parser = ArgumentParser()
parser.add_argument('-t', '--token', type=str, default=os.getenv('TELEGRAM_TOKEN'))
parser.add_argument('-m', '--model', type=str, default='alefiury/wav2vec2-large-xlsr-53-coraa-brazilian-portuguese-gain-normalization')
parser.add_argument('-c', '--cpu', action='store_true', default=not torch.cuda.is_available())
args = parser.parse_args()
print(args)

device = 'cuda' if not args.cpu else 'cpu'

model = Wav2Vec2ForCTC.from_pretrained(args.model).to(device)
processor = Wav2Vec2Processor.from_pretrained(args.model)
vocab_dict = processor.tokenizer.get_vocab()
sorted_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(),key=lambda item: item[1])}

# ???
lm = None
# if self.lm:
#     self.lm_decoder = build_ctcdecoder(
#         list(self.sorted_dict.keys()),
#         self.lm
#     )

def inference(audio):
    audio, sr = torchaudio.load(audio, normalize=False)
    # print('audio', audio, sr)
    audio = torchaudio.functional.resample(audio, sr, 16000); sr = 16000
    audio = torch.mean(audio, axis=0) # convert to mono
    # print('audio_shape', audio.shape)
    
    features = processor(audio, sampling_rate=sr, padding=True, return_tensors='pt')
    input_values = features.input_values.to(device)
    attention_mask = features.attention_mask.to(device)
    predicted = []
    with torch.no_grad():
        logits = model(input_values, attention_mask=attention_mask).logits
    print(logits)
    if lm is not None:
        logits = logits.cpu().numpy()
        for sample_logits in logits:
            predicted.append(lm_decoder.decode(sample_logits))
    else:
        pred_ids = torch.argmax(logits, dim=-1)
        predicted = processor.batch_decode(pred_ids)
    return predicted

async def handle_audio(update: Update, context):
    items = [
        update.message.audio,
        update.message.voice
    ]
    for item in items:
        if item is None:
            continue
        await update.message.chat.send_action(ChatAction.TYPING)
        file = await item.get_file()
        if file.file_size >= 20*1024*1024:
            await update.message.reply_text("Erro: arquivo grande demais. Limite: 20MB", reply_to_message_id=update.message.id)
            continue
        try:
            gc.collect()
            results = inference(file._get_encoded_url())
            gc.collect()
            for result in results:
                await update.message.reply_text(result, reply_to_message_id=update.message.id)
                logger.info(f'audio ({update.message.chat.title}/{update.message.chat.username}-{update.message.chat.id}) {result}')
        except torch.cuda.OutOfMemoryError as e:
            print(e)
            traceback.print_exc()
            await update.message.reply_text("Erro: GPU sem memória o suficiente para realizar a inferência", reply_to_message_id=update.message.id)

app = Application.builder().token(args.token).build()
app.add_handler(MessageHandler(filters.AUDIO | filters.VOICE, handle_audio))
app.run_polling(allowed_updates=Update.MESSAGE)

